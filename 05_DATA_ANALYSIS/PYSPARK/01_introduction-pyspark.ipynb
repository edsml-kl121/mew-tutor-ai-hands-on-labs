{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b333b2ce-5db0-4d07-918c-1a6998d7e4b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"simple-vertex-ai-pipeline\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e9301b5a-6902-48fc-8e3d-7fbdaa988578",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>value</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.369550</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.745123</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.333385</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.066190</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.757993</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999995</th>\n",
       "      <td>9999995</td>\n",
       "      <td>0.872623</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999996</th>\n",
       "      <td>9999996</td>\n",
       "      <td>0.026599</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999997</th>\n",
       "      <td>9999997</td>\n",
       "      <td>0.554922</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999998</th>\n",
       "      <td>9999998</td>\n",
       "      <td>0.194187</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999999</th>\n",
       "      <td>9999999</td>\n",
       "      <td>0.618086</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000000 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id     value category\n",
       "0              0  0.369550        B\n",
       "1              1  0.745123        B\n",
       "2              2  0.333385        B\n",
       "3              3  0.066190        A\n",
       "4              4  0.757993        B\n",
       "...          ...       ...      ...\n",
       "9999995  9999995  0.872623        B\n",
       "9999996  9999996  0.026599        B\n",
       "9999997  9999997  0.554922        A\n",
       "9999998  9999998  0.194187        B\n",
       "9999999  9999999  0.618086        C\n",
       "\n",
       "[10000000 rows x 3 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "size = 10_000_000\n",
    "# Generate dummy data\n",
    "df = pd.DataFrame({\n",
    "    'id': range(size),\n",
    "    'value': np.random.rand(size),\n",
    "    'category': np.random.choice(['A', 'B', 'C'], size=size)\n",
    "})\n",
    "\n",
    "# Save to CSV and Parquet\n",
    "df.to_csv(f'gs://{BUCKET_NAME}/sample.csv', index=False)\n",
    "df.to_parquet(f'gs://{BUCKET_NAME}/sample.partquet', index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "00fcc151-3222-4084-8944-7757897b16ae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas CSV Load Time: 13.84 seconds\n",
      "Pandas Parquet Load Time: 2.56 seconds\n"
     ]
    }
   ],
   "source": [
    "# CSV load time\n",
    "start = time.time()\n",
    "df_csv = pd.read_csv(f'gs://{BUCKET_NAME}/sample.csv')\n",
    "print(\"Pandas CSV Load Time:\", round(time.time() - start, 2), \"seconds\")\n",
    "\n",
    "# Parquet load time\n",
    "start = time.time()\n",
    "df_parquet = pd.read_parquet(f'gs://{BUCKET_NAME}/sample.partquet')\n",
    "print(\"Pandas Parquet Load Time:\", round(time.time() - start, 2), \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "90bccf76-38f0-472a-832b-83577f58d7d4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/25 04:13:58 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark CSV Load Time: 1.52 seconds\n",
      "Spark Parquet Load Time: 0.61 seconds\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import time\n",
    "\n",
    "# Initialize Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"BenchmarkExample\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# CSV load time\n",
    "start = time.time()\n",
    "df_csv = spark.read.option(\"header\", True).csv(f'gs://{BUCKET_NAME}/sample.csv')\n",
    "df_csv.count()  # trigger execution\n",
    "print(\"Spark CSV Load Time:\", round(time.time() - start, 2), \"seconds\")\n",
    "\n",
    "# Parquet load time\n",
    "start = time.time()\n",
    "df_parquet = spark.read.parquet(f'gs://{BUCKET_NAME}/sample.partquet')\n",
    "df_parquet.count()  # trigger execution\n",
    "print(\"Spark Parquet Load Time:\", round(time.time() - start, 2), \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7cd86e64-8aec-405e-81be-9ec9f31bef1c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üêº Pandas CSV Total Time: 10.26 seconds\n",
      "üêº Pandas CSV Total Time: 3.05 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>category</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>A</th>\n",
       "      <td>0.750036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B</th>\n",
       "      <td>0.749853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C</th>\n",
       "      <td>0.749873</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             value\n",
       "category          \n",
       "A         0.750036\n",
       "B         0.749853\n",
       "C         0.749873"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pandas csv\n",
    "start = time.time()\n",
    "df_csv = pd.read_csv(f'gs://{BUCKET_NAME}/sample.csv')\n",
    "df_csv = df_csv[df_csv['value'] > 0.5]\n",
    "df_csv['value_squared'] = df_csv['value'] ** 2\n",
    "result_csv = df_csv.groupby('category').agg({'value': 'mean'}).head(10)\n",
    "pandas_time = round(time.time() - start, 2)\n",
    "print(\"üêº Pandas CSV Total Time:\", pandas_time, \"seconds\")\n",
    "result_csv\n",
    "\n",
    "# Pandas Parquet\n",
    "start = time.time()\n",
    "df_csv = pd.read_parquet(f'gs://{BUCKET_NAME}/sample.partquet')\n",
    "df_csv = df_csv[df_csv['value'] > 0.5]\n",
    "df_csv['value_squared'] = df_csv['value'] ** 2\n",
    "result_csv = df_csv.groupby('category').agg({'value': 'mean'}).head(10)\n",
    "pandas_time = round(time.time() - start, 2)\n",
    "print(\"üêº Pandas CSV Total Time:\", pandas_time, \"seconds\")\n",
    "result_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "119a5761-8546-438d-8e80-6df947c77263",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/25 04:14:13 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark CSV Total Time: 9.97 seconds\n",
      "Spark Parquet Total Time: 1.19 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, pow\n",
    "import time\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Benchmark Spark\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Benchmark: pyspark CSV\n",
    "start = time.time()\n",
    "df_csv = spark.read.option(\"header\", True).csv(f'gs://{BUCKET_NAME}/sample.csv', inferSchema=True)\n",
    "df_csv = df_csv.filter(col(\"value\") > 0.5)\n",
    "df_csv = df_csv.withColumn(\"value_squared\", pow(col(\"value\"), 2))\n",
    "result_csv = df_csv.groupBy(\"category\").avg(\"value\")\n",
    "result_csv.collect()  # Trigger action\n",
    "print(\"Spark CSV Total Time:\", round(time.time() - start, 2), \"seconds\")\n",
    "\n",
    "# Benchmark: pypsark Parquet\n",
    "start = time.time()\n",
    "df_parquet = spark.read.parquet(f'gs://{BUCKET_NAME}/sample.partquet')\n",
    "df_parquet = df_parquet.filter(col(\"value\") > 0.5)\n",
    "df_parquet = df_parquet.withColumn(\"value_squared\", pow(col(\"value\"), 2))\n",
    "result_parquet = df_parquet.groupBy(\"category\").avg(\"value\")\n",
    "result_parquet.collect()\n",
    "print(\"Spark Parquet Total Time:\", round(time.time() - start, 2), \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9d762c78-d6aa-4fe8-bdbc-7d9bba7449b5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 164:============================>                            (4 + 4) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+\n",
      "|category|        avg(value)|\n",
      "+--------+------------------+\n",
      "|       B|0.7498531983885696|\n",
      "|       C|0.7498731103293323|\n",
      "|       A|0.7500361755404709|\n",
      "+--------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "result_csv.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ccf42bb5-a512-4fc2-a208-c926c3c3f909",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+\n",
      "|category|        avg(value)|\n",
      "+--------+------------------+\n",
      "|       B| 0.749853198388569|\n",
      "|       C|0.7498731103293311|\n",
      "|       A|0.7500361755404714|\n",
      "+--------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "result_parquet.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9ca4d3-9041-42ef-9227-b0c64a2fedc8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time before collect (lazy plan only): 5.04 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 172:>                                                        (0 + 8) / 8]\r"
     ]
    }
   ],
   "source": [
    "## What is Lazy Execution\n",
    "\n",
    "# Benchmark: pypsark Parquet\n",
    "start = time.time()\n",
    "df_csv = spark.read.option(\"header\", True).csv(f'gs://{BUCKET_NAME}/sample.csv', inferSchema=True)\n",
    "df_csv = df_csv.filter(col(\"value\") > 0.5)\n",
    "df_csv = df_csv.withColumn(\"value_squared\", pow(col(\"value\"), 2))\n",
    "result_csv = df_csv.groupBy(\"category\").avg(\"value\")\n",
    "lazy_time = time.time()\n",
    "print(\"Time before collect (lazy plan only):\", round(lazy_time - start, 2), \"seconds\")\n",
    "result_csv.collect()\n",
    "total_time = time.time()\n",
    "print(\"Actual execution time (collect only):\", round(total_time - lazy_time, 2), \"seconds\")\n",
    "print(\"Total pipeline time:\", round(total_time - start, 2), \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccdff29a-4754-472f-ab6a-4f48be745096",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark-rt-admin on Serverless Spark (Remote)",
   "language": "python",
   "name": "9c39b79e5d2e7072beb4bd59-runtime-00008197df06"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
